{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get results based on my snp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import sqlite3\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Union, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import requests\n",
    "from IPython.core.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cache_file_name = \"data/vcf_records.parquet.gz\"\n",
    "vcf_file_paths = [Path(f) for f in [\n",
    "    \"/home/s/Dropbox/Siim/health/genetest_2020/GFX0237425.cnv.vcf.gz\",\n",
    "    \"/home/s/Dropbox/Siim/health/genetest_2020/GFX0237425.filtered.indel.vcf.gz\",\n",
    "    \"/home/s/Dropbox/Siim/health/genetest_2020/GFX0237425.filtered.snp.vcf.gz\",\n",
    "    \"/home/s/Dropbox/Siim/health/genetest_2020/GFX0237425.sv.vcf.gz\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_header_line_number(file_name: Union[str, Path], header_pattern: str) -> int:\n",
    "    with gzip.open(str(file_name), \"r\") as f:\n",
    "        line_number = 0\n",
    "        for line in f:\n",
    "            if re.search(header_pattern, line.decode(\"utf-8\")):\n",
    "                return line_number\n",
    "            line_number += 1\n",
    "    raise Exception(f\"Couldn't find header in file {file_name}. Expected header: {header_pattern}\")\n",
    "\n",
    "def get_vcf_file_header_line_number(file_name: Union[str, Path]) -> int:\n",
    "    return get_file_header_line_number(\n",
    "        file_name=file_name,\n",
    "        header_pattern=\"#CHROM\\s+POS\\s+ID\\s+REF\\s+ALT\\s+QUAL\\s+FILTER\\s+INFO\"\n",
    "    )\n",
    "\n",
    "def get_polygenic_score_file_header_line_number(file_name: Union[str, Path]) -> int:\n",
    "    return get_file_header_line_number(\n",
    "        file_name=file_name,\n",
    "        header_pattern=\"rsID\\s+chr_name\\s+chr_position\\s+effect_allele\"\n",
    "    )\n",
    "\n",
    "def read_raw_zipped_vcf_file(file_name: Union[str, Path]) -> pd.DataFrame:\n",
    "    header_row_number = get_vcf_file_header_line_number(file_name=file_name)\n",
    "    result = pd.read_csv(file_name, sep=\"\\s+\", skiprows=header_row_number, dtype=str)\n",
    "    result[\"POS\"] = result[\"POS\"].astype(np.int64)\n",
    "    return result\n",
    "\n",
    "def read_raw_zipped_polygenic_score_file(file_name: Union[str, Path]) -> pd.DataFrame:\n",
    "    header_row_number = get_polygenic_score_file_header_line_number(file_name=file_name)\n",
    "    result = pd.read_csv(file_name, sep=\"\\s+\", skiprows=header_row_number, dtype=str)\n",
    "    result[\"effect_weight\"] = result[\"effect_weight\"].astype(np.float)\n",
    "    result[\"chr_name\"] = result[\"chr_name\"].astype(np.int64)\n",
    "    result[\"chr_position\"] = result[\"chr_position\"].astype(np.int64)\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_vcf_to_df(vcf_files: List[Union[str, Path]], cache_file_name: str = \"data/vcf_records.parquet.gz\"):\n",
    "    if Path(cache_file_name).exists():\n",
    "        return pd.read_parquet(cache_file_name)\n",
    "\n",
    "    dfs = []\n",
    "    for vcf_file_path in vcf_files:\n",
    "        print(f\"Reading in source vcf file {vcf_file_path}\")\n",
    "        dfs.append(read_raw_zipped_vcf_file(vcf_file_path))\n",
    "    raw_vcf_data = pd.concat(dfs, ignore_index=True)\n",
    "    raw_vcf_data.to_parquet(cache_file_name)\n",
    "    return raw_vcf_data\n",
    "\n",
    "def load_polygenic_score_file_to_df(file_name: Union[str, Path]) -> pd.DataFrame:\n",
    "    return read_raw_zipped_polygenic_score_file(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load variant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vcf_df = load_vcf_to_df(vcf_files=vcf_file_paths, cache_file_name=cache_file_name)\n",
    "vcf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df1 = vcf_df.loc[vcf_df[\"POS\"] == 7383583]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load polygenic risk scores to analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Alzheimer's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "polygenic_risk_score_file_alzheimer = \"data/PGS000025.txt.gz\"\n",
    "pgs_025_df = read_raw_zipped_polygenic_score_file(polygenic_risk_score_file_alzheimer)\n",
    "pgs_025_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Schizophrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "polygenic_risk_score_file_schizophrenia = \"data/PGS000133.txt.gz\"\n",
    "pgs_133_df = read_raw_zipped_polygenic_score_file(polygenic_risk_score_file_schizophrenia)\n",
    "pgs_133_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get my genotype for the disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bam_file = \"/home/s/Dropbox/Siim/health/genetest_2020/GFX0237425.bam\"\n",
    "alignment_data = pysam.AlignmentFile(bam_file, \"rb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_read_values_for_allele(chrom: Union[int,str], pos: int) -> Dict[int, List[str]]:\n",
    "    sequence = defaultdict()\n",
    "    for pileupcolumn in alignment_data.pileup(str(chrom), pos - 1, pos + 1):\n",
    "#         print (\"\\ncoverage at base %s = %s\" %\n",
    "#                (pileupcolumn.pos, pileupcolumn.n), \"pileups\", len(pileupcolumn.pileups))\n",
    "        if pos == pileupcolumn.pos + 1: # FIXME: not sure why is +1 needed, found it out based on reports from dantelabs\n",
    "            if len(pileupcolumn.pileups) == 0:\n",
    "                print(f\"Chromosome {chrom} position {pos} does not have any READS\")\n",
    "                continue\n",
    "            reads_at_current_position = []\n",
    "            for pileupread in pileupcolumn.pileups:\n",
    "                if pileupread.is_del:\n",
    "                    reads_at_current_position.append(\"DEL\")\n",
    "                else:\n",
    "#                     print(pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position])\n",
    "#                     print ('\\tbase in read %s = %s' % (pileupread.alignment.query_name, pileupread.alignment.query_sequence[pileupread.query_position]))\n",
    "                    reads_at_current_position.append(pileupread.alignment.query_sequence[pileupread.query_position])\n",
    "            sequence[pos] = reads_at_current_position\n",
    "    return sequence\n",
    "\n",
    "def genotype_from_reads(reads):\n",
    "    counts = {\"A\": 0, \"C\": 0, \"G\": 0, \"T\": 0, \"D\": 0}\n",
    "    for read in reads:\n",
    "        counts[read] += 1\n",
    "    sorted_count_keys = sorted(counts, key=counts.__getitem__, reverse=True)\n",
    "    sorted_count_values = [counts[k] for k in sorted_count_keys]\n",
    "    if sorted_count_values[0] / sum(sorted_count_values) > 0.9:\n",
    "        return f\"{sorted_count_keys[0]}{sorted_count_keys[0]}\"\n",
    "    else:\n",
    "        return f\"{sorted_count_keys[0]}{sorted_count_keys[1]}\"\n",
    "\n",
    "def calculate_chromosome_read_values(loci_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    chromosome_read_values = defaultdict()\n",
    "    for entry in loci_df.to_dict(orient=\"records\"):\n",
    "        chrom = entry[\"chr_name\"]\n",
    "        pos = entry[\"chr_position\"]\n",
    "        if chrom not in chromosome_read_values:\n",
    "            chromosome_read_values[chrom] = {}\n",
    "        allele_read_values = get_read_values_for_allele(chrom, int(pos))\n",
    "\n",
    "        chromosome_read_values[chrom] = {**chromosome_read_values[chrom], **allele_read_values}\n",
    "    return chromosome_read_values\n",
    "\n",
    "def calc_genotypes(loci_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    chromosome_read_values = calculate_chromosome_read_values(loci_df)\n",
    "\n",
    "    seq = pd.DataFrame(columns=[\"chr\",\"pos\",\"genotype\"])\n",
    "    for chrom, pos_reads in chromosome_read_values.items():\n",
    "        for pos, reads in pos_reads.items():\n",
    "            allele = genotype_from_reads(reads)\n",
    "            seq = seq.append({\"chr\": chrom, \"pos\": pos, \"genotype\": allele}, ignore_index=True)\n",
    "    seq[\"genotype\"] = seq[\"genotype\"].astype(str)\n",
    "    return seq\n",
    "\n",
    "def get_my_genotypes_for_pgs(pgs_df: pd.DataFrame, cache_file_name: str, filter: bool = False) -> pd.DataFrame:\n",
    "    cache_file = f\"data/{cache_file_name}\"\n",
    "    if not Path(cache_file).exists():\n",
    "        if filter:\n",
    "            pgs_df_abs_weight = np.abs(pgs_df[\"effect_weight\"])\n",
    "            pgs_df = pgs_df[pgs_df_abs_weight > pgs_df_abs_weight.mean()]\n",
    "        my_genotypes = calc_genotypes(pgs_df)\n",
    "        my_genotypes.to_csv(cache_file, index=None)\n",
    "    else:\n",
    "        my_genotypes = pd.read_csv(cache_file, index_col=None)\n",
    "    return my_genotypes\n",
    "\n",
    "def merge_pgs_with_my_genotype(pgs_df: pd.DataFrame, my_genome_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    merged_df = my_genome_df.merge(pgs_df, left_on=[\"chr\", \"pos\"], right_on=[\"chr_name\", \"chr_position\"])\n",
    "    return merged_df[[\"chr\", \"pos\", \"genotype\", \"effect_allele\", \"reference_allele\", \"effect_weight\"]]\n",
    "\n",
    "def filter_out_none_effect_alleles(merged_pgs_with_my_genotype):\n",
    "    return merged_pgs_with_my_genotype[\n",
    "        (merged_pgs_with_my_genotype[\"genotype\"].map(lambda x: x[0]) == merged_pgs_with_my_genotype[\"effect_allele\"])\n",
    "        | (merged_pgs_with_my_genotype[\"genotype\"].map(lambda x: x[1]) == merged_pgs_with_my_genotype[\"effect_allele\"])\n",
    "    ]\n",
    "\n",
    "def filter_out_effect_alleles(merged_pgs_with_my_genotype):\n",
    "    return merged_pgs_with_my_genotype[\n",
    "        (merged_pgs_with_my_genotype[\"genotype\"].map(lambda x: x[0]) != merged_pgs_with_my_genotype[\"effect_allele\"])\n",
    "        & (merged_pgs_with_my_genotype[\"genotype\"].map(lambda x: x[1]) != merged_pgs_with_my_genotype[\"effect_allele\"])\n",
    "    ]\n",
    "\n",
    "def get_genotype_for_chrom_pos(chrom: str, pos: int) -> str:\n",
    "    reads = get_read_values_for_allele(chrom, pos)\n",
    "    if len(reads) != 0:\n",
    "        return genotype_from_reads(reads[pos])\n",
    "    else:\n",
    "        raise Exception(f\"no reads found for chr{chrom}:{pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"ALPHA-1 ANTITRYPSIN DEFICIENCY\")\n",
    "display(get_genotype_for_chrom_pos(\"14\", 94847386))\n",
    "print(\"Aspirin\")\n",
    "display(get_genotype_for_chrom_pos(\"5\", 179220638))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "my_genotypes_for_pgs_025_cache_file = \"my_genotypes_for_pgs_025.csv\"\n",
    "my_genotypes_for_pgs_025 = get_my_genotypes_for_pgs(pgs_025_df, my_genotypes_for_pgs_025_cache_file)\n",
    "display(my_genotypes_for_pgs_025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "my_genotypes_for_pgs_133_cache_file = \"my_genotypes_for_pgs_133.csv\"\n",
    "my_genotypes_for_pgs_133 = get_my_genotypes_for_pgs(pgs_133_df, my_genotypes_for_pgs_133_cache_file, filter=False)\n",
    "display(my_genotypes_for_pgs_133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Search for Alzheimer's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Polygenic risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pgs_025_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select my alleles for list in PGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_genotypes_for_pgs_025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_alzheimers_snps_df = merge_pgs_with_my_genotype(pgs_025_df, my_genotypes_for_pgs_025)\n",
    "my_alzheimers_snps_df = filter_out_none_effect_alleles(my_alzheimers_snps_df)\n",
    "my_alzheimers_snps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_alzheimers_snps_df[\"effect_weight\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### combined using a weighted sum of allele dosages multiplied by their corresponding effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merge_pgs_with_my_genotype(pgs_025_df, my_genotypes_for_pgs_025)\n",
    "# sum(count effect allele in genotype * effect_weight)\n",
    "merged_df[\"effect_allele_1\"] = merged_df[\"genotype\"].map(lambda x: x[0]) == merged_df[\"effect_allele\"]\n",
    "merged_df[\"effect_allele_2\"] = merged_df[\"genotype\"].map(lambda x: x[1]) == merged_df[\"effect_allele\"]\n",
    "merged_df[\"effect_allele_1\"] = merged_df[\"effect_allele_1\"].astype(int)\n",
    "merged_df[\"effect_allele_2\"] = merged_df[\"effect_allele_2\"].astype(int)\n",
    "merged_df[\"gene_dosage\"] = merged_df[\"effect_allele_1\"] + merged_df[\"effect_allele_2\"]\n",
    "merged_df[\"effect\"] = merged_df[\"gene_dosage\"] * merged_df[\"effect_weight\"]\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[\"effect\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Search for schizophrenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get disease related SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "schizophrenia_snvs_ncbi_response = \"/home/s/src/search_your_dna/.idea/httpRequests/2020-10-28T094336.200.json\"\n",
    "with open(schizophrenia_snvs_ncbi_response) as f:\n",
    "    schizophrenia_data = json.load(f)\n",
    "rsIDs_schizophrenia = schizophrenia_data[\"result\"][\"uids\"]\n",
    "\n",
    "\n",
    "schizophrenia_chr_positions = {}\n",
    "for rsID in rsIDs_schizophrenia:\n",
    "    variant = schizophrenia_data[\"result\"][rsID]\n",
    "    chromosome = variant[\"chr_sort\"].lstrip(\"0\")\n",
    "    position = int(variant[\"location_sort\"].lstrip(\"0\"))\n",
    "    schizophrenia_chr_positions[rsID] = [chromosome, position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select only disease variance that I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "schizophrenia_snv_positions = list(map(lambda x: np.int64(x[1]), filter(lambda x: x[1] != 99999999999999999999, schizophrenia_chr_positions.values())))\n",
    "df_row_selector = vcf_df[\"POS\"].isin(schizophrenia_snv_positions)\n",
    "my_schizophrenia_matches = vcf_df.loc[df_row_selector]\n",
    "my_schizophrenia_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select my alleles for list in PGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_genotypes_for_pgs_133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_schizophrenia_snps_df = merge_pgs_with_my_genotype(pgs_133_df, my_genotypes_for_pgs_133)\n",
    "my_for_schizophrenia_snps_df = filter_out_none_effect_alleles(my_schizophrenia_snps_df)\n",
    "my_against_schizophrenia_snps_df = filter_out_effect_alleles(my_schizophrenia_snps_df)\n",
    "display(my_for_schizophrenia_snps_df)\n",
    "display(my_against_schizophrenia_snps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merge_pgs_with_my_genotype(pgs_133_df, my_genotypes_for_pgs_133)\n",
    "# sum(count effect allele in genotype * effect_weight)\n",
    "merged_df[\"effect_allele_1\"] = merged_df[\"genotype\"].map(lambda x: x[0]) == merged_df[\"effect_allele\"]\n",
    "merged_df[\"effect_allele_2\"] = merged_df[\"genotype\"].map(lambda x: x[1]) == merged_df[\"effect_allele\"]\n",
    "merged_df[\"effect_allele_1\"] = merged_df[\"effect_allele_1\"].astype(int)\n",
    "merged_df[\"effect_allele_2\"] = merged_df[\"effect_allele_2\"].astype(int)\n",
    "merged_df[\"gene_dosage\"] = merged_df[\"effect_allele_1\"] + merged_df[\"effect_allele_2\"]\n",
    "merged_df[\"effect\"] = merged_df[\"gene_dosage\"] * merged_df[\"effect_weight\"]\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df[\"effect\"].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alignment_data.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collect PGS data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get all traits available in pgs catalogue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_all_pgs_api_data(api_endpoint: str):\n",
    "    cache_file = f\"data/pgs_catalog_{api_endpoint.replace('/', '-')}.json\"\n",
    "    if Path(cache_file).exists():\n",
    "        print(f\"Found cache file {cache_file}. Loading data from cache.\")\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    limit = 50\n",
    "    offset = 0\n",
    "    traits = []\n",
    "    while True:\n",
    "        url = f\"https://www.pgscatalog.org/rest/{api_endpoint}?limit={limit}&offset={offset}\"\n",
    "        print(f\"Requesting pgs data from {url}\")\n",
    "        traits_response = requests.get(url=url)\n",
    "        data = traits_response.json()\n",
    "        traits.extend(data[\"results\"])\n",
    "        if data[\"next\"] == None:\n",
    "            break\n",
    "        offset += limit\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(traits, f)\n",
    "    return traits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_traits_result = get_all_pgs_api_data(\"trait/all\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_pgs_traits_df = pd.DataFrame(all_traits_result)\n",
    "print(all_pgs_traits_df.columns)\n",
    "all_pgs_traits_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pgs_ids = set(reduce(lambda a, b: a + b, all_pgs_traits_df[\"associated_pgs_ids\"].to_list(), []))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get pgs entities from pgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_file(url: str, local_filename: str) -> None:\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "def read_or_download_pgs_scoring_file(pgs_id: str):\n",
    "    cache_file = f\"data/{pgs_id}.txt.gz\"\n",
    "    if Path(cache_file).exists():\n",
    "        print(f\"Found cache file {cache_file}. Loading data from cache.\")\n",
    "        return read_raw_zipped_polygenic_score_file(cache_file)\n",
    "    url = f\"https://www.pgscatalog.org/rest/score/{pgs_id}\"\n",
    "    print(f\"Requesting pgs data from {url}\")\n",
    "    data = requests.get(url)\n",
    "    response_data = data.json()\n",
    "    scoring_file_url = response_data[\"ftp_scoring_file\"]\n",
    "    download_file(scoring_file_url, cache_file)\n",
    "    return read_raw_zipped_polygenic_score_file(cache_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Download all pgs scoring files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pgs_id in sorted(pgs_ids):\n",
    "    try:\n",
    "        read_or_download_pgs_scoring_file(pgs_id)\n",
    "        time.sleep(0.5)  # Not to overload api with requests\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong when parsing pgs file\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating snp database\n",
    "#### get all SNP chr/pos values from ncbi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "available for download in: https://ftp.ncbi.nih.gov/snp/organisms/human_9606/VCF/\n",
    "as `00-All.vcf.gz`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Store results in a sqlite db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_rsid_file = \"/home/s/src/search_your_dna/data/00-All.vcf\"\n",
    "vcf_database_file = \"/home/s/src/search_your_dna/data/ncbi_snpdb_all_ids.sqlite\"\n",
    "conn = sqlite3.connect(vcf_database_file)\n",
    "cursor = conn.cursor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cursor.execute('''create table if not exists\n",
    "\tall_snp_pos\n",
    "(\n",
    "\tchrom text,\n",
    "\tpos int\n",
    ")''')\n",
    "cursor.execute('''create unique index if not exists\n",
    "\tall_snp_pos_chrom_pos_index\n",
    "on\n",
    "\tall_snp_pos\n",
    "(\n",
    "\tchrom,\n",
    "\tpos\n",
    ")''')\n",
    "conn.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def chunked_insert(all_values: Iterable, chunk_size = 10000):\n",
    "\n",
    "    all_values = list(all_values)\n",
    "    print(f\"Inserting chrom {all_values[0][0]} values, totalling {len(all_values)}\")\n",
    "    for i in range(0, len(all_values), chunk_size):\n",
    "        if i + chunk_size > len(all_values):\n",
    "            cursor.execute(f\"\"\"INSERT INTO all_snp_pos (chrom,pos) VALUES {all_values[i:].__repr__()[1:-1]};\"\"\")\n",
    "        else:\n",
    "            cursor.execute(f\"\"\"INSERT INTO all_snp_pos (chrom,pos) VALUES {all_values[i:i+chunk_size].__repr__()[1:-1]};\"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "def database_is_already_populated():\n",
    "    cursor.execute(\"SELECT * FROM all_snp_pos limit 1\")\n",
    "    res = cursor.fetchall()\n",
    "    return len(res) != 0\n",
    "\n",
    "def persist_all_snps_to_db(file_name: Union[str, Path]) -> None:\n",
    "\n",
    "    header_pattern = \"#CHROM\\s+POS\\s+ID\\s+REF\\s+ALT\\s+QUAL\\s+FILTER\\s+INFO\"\n",
    "    snps = set()\n",
    "    with open(str(file_name), \"r\") as f:\n",
    "        passed_header = False\n",
    "        last_chrom = \"1\"\n",
    "        for line_text in f:\n",
    "            if not passed_header:\n",
    "                if re.search(header_pattern, line_text):\n",
    "                    passed_header = True\n",
    "            else:\n",
    "                line_parts = line_text.split(\"\\t\")\n",
    "                current_chrom = line_parts[0]\n",
    "                if last_chrom != current_chrom:\n",
    "                    chunked_insert(all_values=snps)\n",
    "                    snps = set()\n",
    "                    last_chrom = current_chrom\n",
    "                snps.add((current_chrom, line_parts[1]))\n",
    "\n",
    "    # persist also final snps\n",
    "    chunked_insert(all_values=snps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "persist_all_snps_to_db(all_rsid_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chrom_list = [\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "    '5',\n",
    "    '6',\n",
    "    '7',\n",
    "    '8',\n",
    "    '9',\n",
    "    '10',\n",
    "    '11',\n",
    "    '12',\n",
    "    '13',\n",
    "    '14',\n",
    "    '15',\n",
    "    '16',\n",
    "    '17',\n",
    "    '18',\n",
    "    '19',\n",
    "    '20',\n",
    "    '21',\n",
    "    '22',\n",
    "    'MT',\n",
    "    'X',\n",
    "    'Y'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_snp_pos = pd.read_sql(\"SELECT distinct (chrom) FROM all_snp_pos\", con=conn)\n",
    "all_snp_pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### find my genotype for all SNP values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### store SNP values in sqlite database\n",
    "\n",
    "### Calculate PGS for my dna\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}